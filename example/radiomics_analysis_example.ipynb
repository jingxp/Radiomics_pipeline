{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <i>Radiomics analysis examples based on classicial machine learning algorithms (non deep learning models) <br>\n",
    "    \n",
    "This tutorial shows how to develop a standard radiomics pipeline for the classification tasks. <br>\n",
    "The following features will be illustrated and dicussed: <br><br>\n",
    "<b>1. Radiomic feature extraction.</b> '<i>example based on a large-scale lung nodule dataset</i>'<br>\n",
    "<b>2. Basic classification models.</b> <br>\n",
    "<b>3. Fine tuning model parameters.</b> <br>\n",
    "<b>4. Class imbalance issues.</b> <br>\n",
    "<b>5. Cross validation analysis</b> <br>\n",
    "<b>6. Dimensionality reduction and feature selection.</b> <br>\n",
    "<b>7. Determining robust radiomics signature.</b> <br>\n",
    "<b>8. Model validation.</b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>1. Radiomic feature extraction <br><br>\n",
    "Extracting the radiomic feature will be done by employing the standard open-source Python package developed for radiomic features in medical imaging known as __[PyRadiomics](https://pyradiomics.readthedocs.io/en/latest/)__. <br><br>\n",
    "The __[Kaggle Data Science Bowl 2017](https://www.kaggle.com/c/data-science-bowl-2017)__ contains a total number of 2101 clinical chest Low-Dose Computed Tomography scans from which 1397, 198, and 506 subjects belong to the training, validation, and test sets, respectively.<br> The goal of the challenge was to automatically predict lung cancer status; for that, each image was labeled as “1” if the patient was diagnosed with lung cancer within one year from the scan and “0” otherwise. Therefore, the idea was to develope a model to predict the binary cancer status.<br> The challenge organizer provided .dcm image data and a .csv file with the corresponding cancer status but not the segmentation masks or coordinates of the lung nodules. <br> From 1397 training subjects, 968 LDCT scans were manually examined by an expert radiologist and 1297 pulmonary nodules were delineated.<br>\n",
    "Therefore, we want to load the original LDCT scans along with their corresponding segmentation masks to extract the radiomic features.<br><br>\n",
    "The following figure shows the 2D axial views of four cases. The first row represents the benign lung nodules while the second rows demonstrates the malignant nodule. As it can be seen from the examples, visually distinguishing these two classes from each other is a demanding task as they shared numerous visual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the required libraries\n",
    "import os\n",
    "import sys\n",
    "import six\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import SimpleITK as sitk\n",
    "from radiomics import featureextractor\n",
    "sys.path.append('/home/umcgdash/xp_jing/density/Radiomics_pipeline/')\n",
    "from data_loader.sitk_stuff import array2sitkimage\n",
    "from data_loader.paths_and_dirs import read_excel_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_root = '/home/umcgdash/xp_jing/density/croped_mri_volumn/'\n",
    "# size_set =set({})\n",
    "# for file in glob.glob(file_root+'*_mask.npz'):\n",
    "#     xx = np.load(file)\n",
    "#     if tuple(xx['arr_0'].shape[1:2]) not in size_set:\n",
    "#         print(file, tuple(xx['arr_0'].shape[1:3]))\n",
    "#         size_set.add(tuple(xx['arr_0'].shape[1:2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_sitk = sitk.GetImageFromArray(xx['arr_0'])\n",
    "# image_sitk.SetOrigin((0, 0, 0))\n",
    "# image_sitk.SetSpacing([1.2,0.4557,0.4557])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(image_sitk.GetSize())\n",
    "# print(image_sitk.GetSpacing())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set a filename for the extracted features.\n",
    "radiomic_feature_name = 'radiomic_features_train.csv'\n",
    "# dir csv file\n",
    "df_path = '../../DATA_FOR_TRAINING_NO_PROTHESIS_WITHPATH.xlsx' \n",
    "# dir to image data\n",
    "data_root = \"../../croped_mri_volumn\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set dir to write the extracted features in a .csv file\n",
    "radiomic_path_write = os.path.join('../features/', radiomic_feature_name)\n",
    "# loading the lung nodule csv file that contains the class labels\n",
    "\n",
    "patient_id = read_excel_file(df_path)\n",
    "data_length = len(patient_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729\n"
     ]
    }
   ],
   "source": [
    "print(data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction parameters:\n",
      "Extraction parameters:\n",
      "Extraction parameters:\n",
      " {'minimumROIDimensions': 2, 'minimumROISize': None, 'normalize': False, 'normalizeScale': 1, 'removeOutliers': None, 'resampledPixelSpacing': None, 'interpolator': 'sitkBSpline', 'preCrop': False, 'padDistance': 5, 'distances': [1], 'force2D': False, 'force2Ddimension': 0, 'resegmentRange': None, 'label': 1, 'additionalInfo': True, 'binWidth': 25, 'weightingNorm': None}\n",
      "Enabled filters:\n",
      "*16 {'Original': {}, 'LoG': {'sigma': [0.5, 1, 1.5]}, 'Wavelet': {}}\n",
      "Enabled features:\n",
      "*16 {'shape': [], 'firstorder': [], 'glcm': [], 'glrlm': [], 'glszm': [], 'gldm': []}\n"
     ]
    }
   ],
   "source": [
    "# Instantiating Radiomics Feature Extraction\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor()\n",
    "# the params.yaml file set the type of features we want to extract.\n",
    "# It is set to extract all the first order, 2nd order, geometric, \n",
    "# wavelet-based features and LoG for the following sigma values [0.5, 1, 1.5]\n",
    "param_path = os.path.join(os.getcwd(), 'params.yaml')\n",
    "extractor = featureextractor.RadiomicsFeatureExtractor(param_path)\n",
    "print('Extraction parameters:''\\n'*3, extractor.settings)\n",
    "print('Enabled filters:\\n*16', extractor.enabledImagetypes)\n",
    "print('Enabled features:\\n*16', extractor.enabledFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 1 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 2 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 3 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 4 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 5 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 6 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 7 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 8 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 9 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 10 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 11 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 12 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 13 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 14 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 15 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n",
      "GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "working on case 16 out of 729:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data will be called one by one and the extracted features will be saved in a .csv file\n",
    "for ind in range(len(patient_id)):\n",
    "    \n",
    "    img_path = os.path.join(data_root, patient_id[ind][0] + '_T1_pre.npz')\n",
    "    mask_path = os.path.join(data_root, patient_id[ind][0] + '_mask.npz')\n",
    "    subject_label = patient_id[ind][1]\n",
    "    \n",
    "    print('\\n'*10)\n",
    "    print('working on case {} out of {}:'.format(ind+1, data_length))\n",
    "    print('\\n'*10)\n",
    "    \n",
    "    \n",
    "    img_ = np.load(img_path)\n",
    "    img,img_size,_,_,_ = array2sitkimage(img_['arr_0'])\n",
    "    \n",
    "    mask_= np.load(mask_path)\n",
    "    mask,mask_size,_,_,_ = array2sitkimage(mask_['arr_0'])\n",
    "    \n",
    "    assert img_size == mask_size\n",
    "    features = extractor.execute(img, mask)\n",
    "    features_all = {}\n",
    "    for key, value in six.iteritems(features):\n",
    "        if key.startswith('original') or key.startswith('wavelet') or \\\n",
    "        key.startswith('log'):\n",
    "            features_all['Subject_ID'] = patient_id[ind][0]\n",
    "            features_all['Subject_Label'] = subject_label\n",
    "            features_all[key] = features[key]\n",
    "            \n",
    "    df = pd.DataFrame(data=features_all,  index=[ind])\n",
    "    if ind == 0:\n",
    "        df.to_csv(radiomic_path_write, mode='a')\n",
    "    else:\n",
    "        df.to_csv(radiomic_path_write, header = None, mode='a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the following setting a total number of 1070 features were extracted from each subject.<br>\n",
    "The structure of the 'radiomic_features_test.csv' file is:\n",
    "\n",
    "number | Subject_ID | Subject_Label | original_shape_Elongation |original_shape_Flatness | other features... |\n",
    ":---|:---:|:---:|:---:|:---:|:---:|\n",
    "0 | c928b4fee44ea322ac65348ae2ff20b8_cropped | 0 | 0.576 | 0.540 | feature values... |\n",
    "1 | 91ee390b30927af8804d1f2adf2aefa8_cropped | 0 | 0.849 | 0.673 | feature values... |\n",
    "2 |  76fff2029e577190ce0bf070192b889e_cropped - Copy |0 |0.8448 | 0.722|feature values... |\n",
    "3 |f43843b39a5be8cd30dab59bd11499ed_cropped - Copy|0|0.789|0.521|feature values... |\n",
    "4 |184fa4ae2b7ae010625d89f10186f1c5_cropped|1|0.913|0.894|feature values... |\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>2. Basic classification<br><br>\n",
    "\n",
    "The next step is to develop a predictive model. In this repo we employ the __[SKLEARN](https://scikit-learn.org/stable/)__ library for preprocessing the data, developing the predictive models, training and testing the compiled models. <br>\n",
    "Some of the most conventional learning algorithms were scripted in the '../learning_algorithms/models.py'. The training procedures of the models is already developed in the '../training/trainer.py' <br><br>\n",
    "Generally, building a predictive model consists of these steps:<br>\n",
    "1. Loading the feature matrix\n",
    "2. Shuffling the order of the data\n",
    "3. Splitting the data into train and test subsets.\n",
    "4. Train the model on the training subset\n",
    "5. Evaluate the performance of the learned model on the test subset. <br>\n",
    "\n",
    "Implementation of these steps would be like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('../../radiomics_pipeline/')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from training.trainer import learning\n",
    "from learning_algorithms.models import decicion_tree\n",
    "from learning_algorithms.models import random_forest, naive\n",
    "from learning_algorithms.models import knn, adab_tree, lda, qda\n",
    "from sklearn.model_selection import train_test_split\n",
    "from data_loader.feature_loader import load_radiomic_set\n",
    "from utilities.feature_tools import feature_normalization, data_shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the radiomics features\n",
    "path_to_radiomics = '../features/radiomic_features_test.csv'\n",
    "subject_ids ,features_names, feature_set, label_set = load_radiomic_set(path_to_radiomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly shuffle the order of the data\n",
    "seed_val = 70 # a random integer\n",
    "feature_set, label_set = data_shuffling(feature_set, label_set, seed_val)\n",
    "print('the size of the feature matrix is {}'.format(feature_set.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into subsets\n",
    "x_train, x_val, y_train, y_val = train_test_split(feature_set,\n",
    "                                                    label_set,\n",
    "                                                    test_size=0.2, # assigning 20% of the data to the test set.\n",
    "                                                    random_state=None)\n",
    "print('the size of the training subset is {}'.format(x_train.shape))\n",
    "print('the size of the validation subset is {}'.format(x_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compiling the model with default settings\n",
    "clf_dt = decicion_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the compiled model with train data and evaluate it on the validation set.\n",
    "clf_dt_summary = learning(clf_dt, x_train, y_train, x_val, y_val, x_test=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following evaluation metrics are stored in the clf_summary dictionary:<br>\n",
    "accuracy, sensitivity, specificty, AUROC of the validation set.\n",
    "Moreover, the predicted probability of the validation sets, for each single subject, is stored\n",
    "for further statistical analyses, if needed.<br>\n",
    "In addition, if the x_test is provided and is not None, the predicted probabilities of the test\n",
    "set will be stored as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The acc, sen, spc, and auc values of the validation set are {a:1.3f},{b:1.3f}, {c:1.3f} and {d:1.3f}'.format(\n",
    "      a=clf_dt_summary['val_accuracy'], b=clf_dt_summary['val_sensitivity'],\n",
    "      c=clf_dt_summary['val_specificity'], d=clf_dt_summary['val_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets try the previous steps with other learning algorithms as well.<br>\n",
    "This time, we use the Random Forest, KNN and adaptive boosting, LDA, QDA and naive models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf = random_forest()\n",
    "clf_knn = knn()\n",
    "clf_adab = adab_tree()\n",
    "clf_lda = lda()\n",
    "clf_qda = qda()\n",
    "clf_naive = naive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_summary = learning(clf_rf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_lda_summary = learning(clf_lda, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_qda_summary = learning(clf_qda, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_adab_summary = learning(clf_adab, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_knn_summary = learning(clf_knn, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_naive_summary = learning(clf_naive, x_train, y_train, x_val, y_val, x_test=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the performance of the models on the same data with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_summary = {'AUC':[clf_rf_summary['val_auc'],clf_knn_summary['val_auc'],\n",
    "                         clf_adab_summary['val_auc'],clf_lda_summary['val_auc'], clf_qda_summary['val_auc'],\n",
    "                          clf_naive_summary['val_auc'],clf_dt_summary['val_auc']],\n",
    "                  'ACC':[clf_rf_summary['val_accuracy'],\n",
    "                          clf_knn_summary['val_accuracy'], clf_adab_summary['val_accuracy'],\n",
    "                          clf_lda_summary['val_accuracy'], clf_qda_summary['val_accuracy'],\n",
    "                          clf_naive_summary['val_accuracy'],clf_dt_summary['val_accuracy']]}\n",
    "\n",
    "pd.DataFrame.from_dict(compare_summary, orient='index',\n",
    "                       columns=['RF', 'KNN', 'Adab', 'LDA', 'QDA', 'Naive', 'DT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>3. Fine-tuning the models to improve the performance<br><br>\n",
    "In the previous section, several learning models were employed with default settings.<br>\n",
    "In the following we invetigate the effects of parameters tuning on the model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by tuning the Random Forest parameters. <br>\n",
    "The funciton 'random_forest(n_estimators=10, criterion='gini', max_depth=5, class_weight=None)' gets four arguments.<bt> The idea is to change these parameters to find out whether they would improve the model performance or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by changing the number of estimators:\n",
    "estimators = range(1,100)\n",
    "rf_auc_estimators = {}\n",
    "for n_estimator in estimators:\n",
    "    clf_rf = random_forest(n_estimators=n_estimator, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_rf_summary = learning(clf_rf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    rf_auc_estimators[n_estimator] = clf_rf_summary['val_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_estimate_x_axis = list(rf_auc_estimators.keys())\n",
    "rf_estimate_y_axis = list(rf_auc_estimators.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rf_estimate_x_axis, rf_estimate_y_axis, color='blue')\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"# of estimators\")\n",
    "plt.ylabel(\"Predicted AUC values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a fixed number of estimators we change the max_depth of the trees:\n",
    "depths = range(1,50)\n",
    "rf_auc_depth = {}\n",
    "for n_depth in depths:\n",
    "    clf_rf = random_forest(n_estimators=20, criterion='gini', max_depth=n_depth, class_weight=None)\n",
    "    clf_rf_summary = learning(clf_rf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    rf_auc_depth[n_depth] = clf_rf_summary['val_auc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_depth_x_axis = list(rf_auc_depth.keys())\n",
    "rf_depth_y_axis = list(rf_auc_depth.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(rf_depth_x_axis, rf_depth_y_axis, color='red')\n",
    "plt.title(\"Random Forest\")\n",
    "plt.xlabel(\"depth of estimators\")\n",
    "plt.ylabel(\"Predicted AUC values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be observed, changing the model parameters would lead to significant changes in the performance of the models. However, the optimum values of such parameters are usually calculated experimentally. <br>\n",
    "As an another example, we conduct the same type of experiments with Adab model. <br>\n",
    "Emloying the _[SKLEARN Grid Search](https://scikit-learn.org/stable/modules/grid_search.html)_ is an standard way to determine the best settings of hyperparameters for a certain model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the depth of the trees:\n",
    "adab_depth = range(1,30)\n",
    "adab_auc_depth = {}\n",
    "for n_depth in adab_depth:\n",
    "    clf_adab = adab_tree(max_depth=n_depth, criterion='gini', class_weight=None, n_estimators=10)\n",
    "    clf_adab_summary = learning(clf_adab, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    adab_auc_depth[n_depth] = clf_adab_summary['val_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab_depth_x_axis = list(adab_auc_depth.keys())\n",
    "adab_depth_y_axis = list(adab_auc_depth.values())\n",
    "plt.plot(adab_depth_x_axis, adab_depth_y_axis, color='green')\n",
    "plt.title(\"Adaptive Boosting\")\n",
    "plt.xlabel(\"depth of estimators\")\n",
    "plt.ylabel(\"Predicted AUC values\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the number of estimators:\n",
    "adab_estimator = range(1,50)\n",
    "adab_auc_estimator = {}\n",
    "for n_estimate in adab_estimator:\n",
    "    clf_adab = adab_tree(max_depth=7, criterion='gini', class_weight=None, n_estimators=n_estimate)\n",
    "    clf_adab_summary = learning(clf_adab, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    adab_auc_estimator[n_estimate] = clf_adab_summary['val_auc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adab_depth_x_axis = list(adab_auc_estimator.keys())\n",
    "adab_depth_y_axis = list(adab_auc_estimator.values())\n",
    "plt.plot(adab_depth_x_axis, adab_depth_y_axis, color='gray')\n",
    "plt.title(\"Adaptive Boosting\")\n",
    "plt.xlabel(\"depth of estimators\")\n",
    "plt.ylabel(\"Predicted AUC values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>4. Class imbalance issue<br><br>\n",
    "\n",
    "Generally speaking, in the previous experiments, we did not examine whether the class labels are balanced or not.\n",
    "Therefore, the models were compiled assuming that there are equal number of datapoints in each class. However, class imbalance issue poses a challenge to the predictive models and, in general, degrade the model performance especially for the minority class. To tackle this issue, different strategies have been proposed including:<br>\n",
    "-  oversampling from minority class\n",
    "-  undersampling from majority class\n",
    "-  modifying the objective(loss) function\n",
    "-  adding class weights\n",
    "-  synthesizing new data for minority class <br>\n",
    "\n",
    "In the following, we investigate the effect of \"adding class weights\" and \"synthesizing new data\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we examine the distribution of the class labels in the dataset:\n",
    "label_set_tuple = tuple(label_set)\n",
    "benign_frequency = label_set_tuple.count(0)\n",
    "malignant_frequency = label_set_tuple.count(1)\n",
    "print('From {} data {} are benign and {} are labeled as malignant'.format(\n",
    "    len(label_set_tuple),benign_frequency, malignant_frequency))\n",
    "data_labels = ('Benign', 'Malignant')\n",
    "values = [benign_frequency, malignant_frequency]\n",
    "explode = (0, 0.1)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(values, explode=explode, labels=data_labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=45)\n",
    "ax1.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding class weights as a dictionary and compare against the same model but without class weights.\n",
    "clf_rf_base = random_forest(n_estimators=100, criterion='gini', max_depth=15, class_weight=None)\n",
    "clf_rf_class_weight = random_forest(n_estimators=100, criterion='gini', max_depth=15, class_weight={0:1,1:2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_base_summary = learning(clf_rf_base, x_train, y_train, x_val, y_val, x_test=None)\n",
    "clf_rf_class_weight_summary = learning(clf_rf_class_weight, x_train, y_train, x_val, y_val, x_test=None)\n",
    "print('the performance of the base model is {a:1.3f} and the model with added class weight is {b:1.3f}'.format(\n",
    "    a = clf_rf_base_summary['val_auc'], b = clf_rf_class_weight_summary['val_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can bee seen, although class weight is an efficient strategy, but yet I won't lead to a remarkable improvement of the model performance.<br>\n",
    "As an alternative, we can employ the 'Synthetic Minority Oversampling Technique <b>(SMOTE)</b>' algorithm to synthetically generate new data for the minority class that eventually balance the class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing.class_balancing import smote_balancing\n",
    "feature_set_balanced, label_set_balanced = smote_balancing(feature_set, label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_set_balanced_tuple = tuple(label_set_balanced)\n",
    "benign_balanced = label_set_balanced_tuple.count(0)\n",
    "malignant_balanced = label_set_balanced_tuple.count(1)\n",
    "data_labels = ('Benign', 'Malignant')\n",
    "values = [benign_balanced, malignant_balanced]\n",
    "explode = (0, 0.1)\n",
    "\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(values, explode=explode, labels=data_labels, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=45)\n",
    "ax1.axis('equal')  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the data into subsets\n",
    "x_train, x_val, y_train, y_val = train_test_split(feature_set_balanced,\n",
    "                                                    label_set_balanced,\n",
    "                                                    test_size=0.2, # assigning 20% of the data to the test set.\n",
    "                                                    random_state=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_rf_base = random_forest(n_estimators=100, criterion='gini', max_depth=15, class_weight=None)\n",
    "clf_rf_base_summary = learning(clf_rf_base, x_train, y_train, x_val, y_val, x_test=None)\n",
    "print('the performance of the base model with balanced data set is {}'.format(\n",
    "    clf_rf_base_summary['val_auc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>5. Important role of cross validation <br><br>\n",
    "    \n",
    "Even though balancing the feature sets led to significant improvement in the model performance, we should not\n",
    "rely only on the achieved values. It would just happened by chance that well-handled data were splited to validation set. To tackle this issue, it has been highly recommended to perform the analyses by following the cross validation approach.<br>\n",
    "In the following, we perform the k-fold cross validation experiments on the balanced dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from utilities.feature_tools import train_val_split, cross_val_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 150\n",
    "n_fold_split = 5\n",
    "\n",
    "feature_set_balanced, label_set_balanced = data_shuffling(feature_set_balanced, label_set_balanced, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_set_balanced, label_set_balanced, train_index, val_index)    \n",
    "\n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    fold_stats[fold_name] = clf_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "\n",
    "compare_summary = {'Accuracy_mean':mean_acc,'Sensitivity_mean':mean_sen,\n",
    "                  'Specificity_mean': mean_spc,'AUC_mean': mean_auc}\n",
    "\n",
    "pd.DataFrame.from_dict(compare_summary, orient='index',\n",
    "                       columns=['Evaluation metrics of cross validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The observed results assure us that the predicted results were not achieved by chance. However, this argument\n",
    "will be more strengthen if we repeat the cross validation several times after randomly shuffle the data. This will be detailed in section 7. <br>\n",
    "In some cases, we also need to <b>normalize the data</b> before training the models.<br>\n",
    "In specific, in radiomic feature pools,different features have different range of values that would significantly differ from each other. There are a number of ways to normalize the feature values and rescale them into a fixed range like [0,1].<br> What is important is to normalize the training data and, then, use the normalization parameters of the training set for rescaling the validation and test sets:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.feature_tools import feature_normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_set_balanced, label_set_balanced, train_index, val_index)\n",
    "    \n",
    "    x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "\n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    fold_stats[fold_name] = clf_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "\n",
    "compare_summary = {'Accuracy_mean':mean_acc,'Sensitivity_mean':mean_sen,\n",
    "                  'Specificity_mean': mean_spc,'AUC_mean': mean_auc}\n",
    "\n",
    "pd.DataFrame.from_dict(compare_summary, orient='index',\n",
    "                       columns=['Evaluation metrics of cross validation'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>6. Feature selection and dimensionality reduction <br><br>\n",
    "\n",
    "It would be possible that many of extracted radiomic features are linearly/non-linearly associated to each other.\n",
    "In addition, some features would not be informative and their presence just confuse the learning algorithms.\n",
    "Moreover, in dealing with the limited number of data, large number of features would increase the risk of overfitting.<br>\n",
    "Accordingly, one practical approach is to reduce the dimensionality of the featur sets by removing non-informative features.<br>\n",
    "In a broad definition, two of the most conventional feature selection families are known as <b>filter-based methods</b> and <b>'wrapper-based methods'</b>. <br>\n",
    "In this repo the following filter methods are implemented:<br>\n",
    "- constant: <i>removing constant features</i><br>\n",
    "- uncorrelated: <i>removing correlated(highly correlated) features</i><br>\n",
    "- lasso: <i>Least Absolute Shrinkage and Selection Operator</i><br>\n",
    "- reliefF: <i>Extended Relevance in Estimatory Features (RELIEF)</i><br>\n",
    "- pca: <i>dimensionality redocution with linear/kernel-based PCA</i><br>\n",
    "- mutual_info: <i>removing nonlinearly correlated features</i><br>\n",
    "And<br>\n",
    "- sequential feature selection: <i>as a wrapper method<i/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_selections.feature_selectors import lasso, relief, pca_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of feature selection with LASSO\n",
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_set_balanced, label_set_balanced, train_index, val_index)\n",
    "    \n",
    "    x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "    \n",
    "    feature_selector = lasso(x_train, y_train, x_val, x_test=None, n_fold=5, max_iters=50, thr=0.5)\n",
    "    x_train = feature_selector['train']\n",
    "    x_val = feature_selector['val']\n",
    "    feature_indices = feature_selector['feature_indices']\n",
    "    fold_name_features = fold_name+'_selected_features'\n",
    "    fold_stats[fold_name_features] = feature_indices\n",
    "    \n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "print('the average AUC value of {} fold cross validation with LASSO feature selection is: {}'.format(n_fold_split, mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of relief feature selection\n",
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_set_balanced, label_set_balanced, train_index, val_index)\n",
    "    \n",
    "    x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "    \n",
    "    feature_selector = relief(x_train, y_train, x_val, x_test = None, n_neighbors=5, n_features=10)\n",
    "    x_train = feature_selector['train']\n",
    "    x_val = feature_selector['val']\n",
    "    \n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "print('the average auc value of {} fold cross validation with RELIEFF feature selection is: {}'.format(n_fold_split, mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of PCA-based dimensionality reduction\n",
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_set_balanced, label_set_balanced, train_index, val_index)\n",
    "    \n",
    "    x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "    \n",
    "    feature_selector = pca_linear(x_train, x_val, x_test = None, n_component=25)\n",
    "    x_train = feature_selector['train']\n",
    "    x_val = feature_selector['val']\n",
    "    \n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "    fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "print('the average auc value of {} fold cross validation with PCA dim reduction is: {}'.format(n_fold_split, mean_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the effect of the number of PCA components on the prediction power:\n",
    "n_component = range(2,31)\n",
    "pca_stats = {}\n",
    "for pca_component in n_component:\n",
    "    print('Working on 5fold cross val with {} number of PCA components'.format(pca_component))\n",
    "    kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "    fold_stats = {}\n",
    "    for train_index, val_index in kf.split(label_set):       \n",
    "        x_train, x_val, y_train, y_val = train_val_split(\n",
    "            feature_set_balanced, label_set_balanced, train_index, val_index)\n",
    "\n",
    "        x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "\n",
    "        feature_selector = pca_linear(x_train, x_val, x_test = None, n_component=pca_component)\n",
    "        x_train = feature_selector['train']\n",
    "        x_val = feature_selector['val']\n",
    "\n",
    "        clf = random_forest(n_estimators=100, criterion='gini', max_depth=5, class_weight=None)\n",
    "        clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test=None)\n",
    "        fold_stats[fold_name] = clf_summary\n",
    "\n",
    "    _, _, _, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "    pca_stats[pca_component] = mean_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_component_x_axis = list(pca_stats.keys())\n",
    "rf_score_y_axis = list(pca_stats.values())\n",
    "plt.plot(pca_component_x_axis, rf_score_y_axis, color='blue')\n",
    "plt.title(\"RF with PCA as feature selection\")\n",
    "plt.xlabel(\"# of PCA components\")\n",
    "plt.ylabel(\"Predicted AUC values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that filter-based feature selection methods will not necessarily lead to improving\n",
    "the prediction performance. In other words the filter-based methods aims to eliminating redundant features (w.r.t to class labels). However, <b>Sequential Feature Selection (SFS)</b> is a method that\n",
    "aims to select a subset of the features with the highest prediction score. <br>\n",
    "Depending on the size of the feature set (number of data and number of features), length of the feature subset to be selected, and the computational power (number of parallel threads) executing the SFS method would take from <b>a couple of minutes to a few days</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_radiomics = '../features/radiomic_features_test.csv'\n",
    "subject_ids ,features_names, feature_set, label_set = load_radiomic_set(path_to_radiomics)\n",
    "feature_set, label_set = smote_balancing(feature_set, label_set)\n",
    "seed_val = 40\n",
    "feature_set, label_set = data_shuffling(feature_set, label_set, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of using SFS integrated with RF learning algorithm\n",
    "from feature_selections.feature_selectors import sequential_selection\n",
    "from training.trainer import learning_with_sfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: finding the most informative subset of features with SFS\n",
    "\n",
    "n_features = 10 # define the number of features to be selected\n",
    "k_fold = 5    # cross validation for feature selection\n",
    "\n",
    "clf_model = random_forest()\n",
    "sfs_model = sequential_selection(clf_model, n_features=n_features, foward_state=True, floating_state=False,\n",
    "                         metric='roc_auc', k_fold=k_fold, n_jobs=-1)\n",
    "\n",
    "feature_set_norm, _, _ = feature_normalization(feature_set, x_val = None, x_test = None)\n",
    "print('Sequential feature selection is in progress ...')\n",
    "sfs_summary = learning_with_sfs(sfs_model, feature_set_norm, label_set, n_features=n_features)\n",
    "print('Done with sequential feature selection step!')\n",
    "exp_name = str(n_features) + ' selected features_names'\n",
    "selected_features = list(sfs_summary[exp_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_feature_name = [features_names[index] for index in selected_features]\n",
    "print('The indices of selected features are {}.'.format(\n",
    "    selected_features, selected_feature_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 2: using only selected features for cross validation\n",
    "seed_val = 40\n",
    "feature_set, label_set = data_shuffling(feature_set, label_set, seed_val)\n",
    "feature_subset = feature_set[:, selected_features]\n",
    "#feature_subset = feature_set\n",
    "n_fold_split = 5\n",
    "fold_num = 0\n",
    "kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "fold_stats = {}\n",
    "for train_index, val_index in kf.split(label_set):\n",
    "    fold_num += 1\n",
    "    fold_name = 'fold_'+str(fold_num)\n",
    "    print('Working on fold: {}'.format(fold_num))\n",
    "    \n",
    "    x_train, x_val, y_train, y_val = train_val_split(\n",
    "        feature_subset, label_set, train_index, val_index)\n",
    "    \n",
    "    x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "            \n",
    "\n",
    "    clf = random_forest(n_estimators=100, criterion='gini', max_depth=300, class_weight=None)\n",
    "    clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test = None)\n",
    "    fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "mean_acc, mean_sen, mean_spc, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "print('\\n'*2)\n",
    "print('the average auc value of {} fold cross validation after selecting {} features with \\\n",
    "SFS algorithm is: {a:1.3f}'.format(n_fold_split,n_features, a=mean_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>7. Determining robust radiomic signature <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we used SFS method to select a subset of features that would result in a robust prediction score. In specific, we, first, conducted the SFS method to identify the most informative feature regardless of their dependency. Then, we used only the selected features to train the same learning algorithm\n",
    "by which the features were selected. These steps were executed only once. However, in real practice, it is strongly recommended to repeat the steps several times after randomly shuffle the data. In other words, after identifying the top informative features, we use the selected features to train a model several times by running the the cross validation approach with random shuffling the data. The evaluation metics, then, should be averaged\n",
    "over all the experiments as the final scores. If no significant deviation of the scres observed in the repeated experiments, the selected subset of radiomic features can be considered as <b>robust radiomic signatures</b>. <br>\n",
    "As an example, we provide a comprehensive instance of such approach that contains almost all the previous steps of this jupyter file:<br>\n",
    "\n",
    "1- loading the feature set<br>\n",
    "2- balancing the feature set<br>\n",
    "3- feature selection with SFS<br>\n",
    "4- shuffle the order of the data<br>\n",
    "5- feature normalization<br>\n",
    "6- cross validation with the selected features<br>\n",
    "7- repeat steps 4 to 6 several times\n",
    "8- averaging the evaludation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('../../radiomics_pipeline/')\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from training.trainer import learning, learning_with_sfs\n",
    "from learning_algorithms.models import random_forest\n",
    "from feature_selections.feature_selectors import sequential_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from data_loader.feature_loader import load_radiomic_set\n",
    "from preprocessing.class_balancing import smote_balancing\n",
    "from utilities.feature_tools import data_shuffling, cross_val_stats\n",
    "from utilities.feature_tools import feature_normalization, train_val_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_radiomics = '../features/radiomic_features_test.csv'\n",
    "subject_ids ,features_names, feature_set, label_set = load_radiomic_set(path_to_radiomics)\n",
    "feature_set, label_set = smote_balancing(feature_set, label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 42\n",
    "feature_set, label_set = data_shuffling(feature_set, label_set, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 10\n",
    "k_fold = 5 # number of folds for feature selection\n",
    "n_fold_split = 5 # number of folds for cross validation after feature selection\n",
    "\n",
    "clf_model = random_forest(n_estimators=50, criterion='gini', max_depth=10, class_weight=None)\n",
    "sfs_model = sequential_selection(clf_model, n_features=n_features, foward_state=True, floating_state=False,\n",
    "                         metric='roc_auc', k_fold=k_fold, n_jobs=-1)\n",
    "\n",
    "feature_set_norm, _, _ = feature_normalization(feature_set, x_val = None, x_test = None)\n",
    "print('Sequential feature selection is in progress ...')\n",
    "sfs_summary = learning_with_sfs(sfs_model, feature_set_norm, label_set, n_features=n_features)\n",
    "print('Done with sequential feature selection step!')\n",
    "exp_name = str(n_features) + ' selected features_names'\n",
    "selected_features = list(sfs_summary[exp_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seeds = np.random.randint(low=0, high=2021,size=100) # generating 100 random values for initial seeds\n",
    "temp_score_seeds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for seed_val in rand_seeds:\n",
    "    counter += 1\n",
    "    print('Working on seed number:{}'.format(counter))\n",
    "\n",
    "    feature_set_copy = copy.deepcopy(feature_set)\n",
    "    subject_labels_copy = copy.deepcopy(label_set)\n",
    "    top_features = selected_features\n",
    "    feature_subset = feature_set_copy[:, top_features]\n",
    "    length = np.arange(subject_labels_copy.shape[0])\n",
    "       \n",
    "    np.random.seed(seed_val)  \n",
    "    np.random.shuffle(length)\n",
    "    feature_subset = feature_subset[length]\n",
    "    labels_subset = subject_labels_copy[length]\n",
    "     \n",
    "    seed_val = str(seed_val)\n",
    "    kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "    fold_stats = {}\n",
    "    fold_num = 0\n",
    "    for train_index, val_index in kf.split(label_set):\n",
    "        fold_num += 1\n",
    "        fold_name = 'fold_'+str(fold_num)\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = train_val_split(\n",
    "            feature_subset, labels_subset, train_index, val_index)\n",
    "    \n",
    "        x_train, x_val, _ = feature_normalization(x_train, x_val, x_test = None)\n",
    "\n",
    "        clf = random_forest(n_estimators=100, criterion='gini', max_depth=300, class_weight=None)\n",
    "        clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test = None)\n",
    "        fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "    _, _, _, mean_auc, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "   \n",
    "    temp_score_seeds[seed_val] = mean_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_auc_values = list(temp_score_seeds.values())\n",
    "mean_auc_scores = np.mean(np.array(predicted_auc_values))\n",
    "std_auc_scores = np.std(np.array(predicted_auc_values))\n",
    "print('running 5fold cv for 100 times resulted an AUC average score of {a:1.3f} with {b:1.3f} standard deviation' \\\n",
    "      .format(a=mean_auc_scores,b=std_auc_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the observed results, now, we can consider the selected radiomic features \"selected_features\" as radiomic signatures for this feature set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>8. Final notes on model validation <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some tasks such as challenges, in addition to train and validation sets, test set would be available as well.\n",
    "In such cases, we often don't have the class labels(targets) of the test set and the task is to develop a predictive model by using train and validation sets and validate the developed model on the test set.<br>\n",
    "In the following we simulate this practice:<br>\n",
    "1- loading the feature set<br>\n",
    "2- balancing the feature set<br>\n",
    "3- shuffle the order of the data<br>\n",
    "4- splitting the feature set into train and test sets<br>\n",
    "5- building a predictive model by using train set<br>\n",
    "$\\;\\;\\;\\;\\;\\;$the train set will be splitted into train and validation subsets <br>\n",
    "6- the model with the best performance over the validation subset is selected to predict the labels of unseen (test) dataset.<br>\n",
    "7- the predicted probabilities of the test set will be compared against the class labels of test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sys.path.append('../../radiomics_pipeline/')\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "from training.trainer import learning, learning_with_sfs\n",
    "from learning_algorithms.models import random_forest\n",
    "from feature_selections.feature_selectors import sequential_selection\n",
    "from sklearn.model_selection import KFold\n",
    "from data_loader.feature_loader import load_radiomic_set\n",
    "from preprocessing.class_balancing import smote_balancing\n",
    "from utilities.feature_tools import data_shuffling, cross_val_stats\n",
    "from utilities.feature_tools import feature_normalization, train_val_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_radiomics = '../features/radiomic_features_test.csv'\n",
    "subject_ids ,features_names, feature_set, label_set = load_radiomic_set(path_to_radiomics)\n",
    "feature_set, label_set = smote_balancing(feature_set, label_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 180 # set a random integer\n",
    "feature_set, label_set = data_shuffling(feature_set, label_set, seed_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign 15% of all data to test set and use the rest as train/validation sets.\n",
    "feature_subset, x_test, label_subset, y_test = train_test_split(feature_set, label_set, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset of features\n",
    "n_features = 10\n",
    "k_fold = 5\n",
    "n_fold_split = 5\n",
    "\n",
    "clf_model = random_forest(n_estimators=50, criterion='gini', max_depth=10, class_weight=None)\n",
    "sfs_model = sequential_selection(clf_model, n_features=n_features, foward_state=True, floating_state=False,\n",
    "                         metric='roc_auc', k_fold=k_fold, n_jobs=-1)\n",
    "\n",
    "feature_set_norm, _, _ = feature_normalization(feature_subset, x_val = None, x_test = None)\n",
    "print('Sequential feature selection is in progress ...')\n",
    "sfs_summary = learning_with_sfs(sfs_model, feature_set_norm, label_subset, n_features=n_features)\n",
    "print('Done with sequential feature selection step!')\n",
    "exp_name = str(n_features) + ' selected features_names'\n",
    "selected_features = list(sfs_summary[exp_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seeds = np.random.randint(low=0, high=2021,size=100) # generating 100 random values for initial seeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross validating several times with only selected features\n",
    "temp_score_seeds = {}\n",
    "seed_summary = {}\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for seed_val in rand_seeds:\n",
    "    counter += 1\n",
    "    print('Working on seed number:{}'.format(counter))\n",
    "\n",
    "    feature_set_copy = copy.deepcopy(feature_subset)\n",
    "    subject_labels_copy = copy.deepcopy(label_subset)\n",
    "    x_test_copy = copy.deepcopy(x_test)\n",
    "    \n",
    "    top_features = selected_features\n",
    "    train_val_set = feature_set_copy[:, top_features]\n",
    "    x_test_copy = x_test_copy[:, top_features]\n",
    "    length = np.arange(subject_labels_copy.shape[0])\n",
    "       \n",
    "    np.random.seed(seed_val)  \n",
    "    np.random.shuffle(length)\n",
    "    train_val_set = train_val_set[length]\n",
    "    train_val_labels = subject_labels_copy[length]\n",
    "     \n",
    "    seed_val = str(seed_val)\n",
    "    kf = KFold(n_splits = n_fold_split, shuffle = False) \n",
    "    fold_stats = {}\n",
    "    fold_num = 0\n",
    "    for train_index, val_index in kf.split(train_val_labels):\n",
    "        fold_num += 1\n",
    "        fold_name = 'fold_'+str(fold_num)\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = train_val_split(\n",
    "            train_val_set, train_val_labels, train_index, val_index)\n",
    "    \n",
    "        x_train, x_val, x_unseen = feature_normalization(x_train, x_val, x_test = x_test_copy)\n",
    "\n",
    "        clf = random_forest(n_estimators=100, criterion='gini', max_depth=300, class_weight=None)\n",
    "        clf_summary = learning(clf, x_train, y_train, x_val, y_val, x_test = x_unseen)\n",
    "        fold_stats[fold_name] = clf_summary\n",
    "    \n",
    "    _, _, _, mean_auc_val, _, _, _, _ = cross_val_stats(fold_stats)\n",
    "   \n",
    "    temp_score_seeds[seed_val] = mean_auc_val\n",
    "    seed_summary[seed_val] = fold_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "employed_seed_vals = list(temp_score_seeds.keys())    \n",
    "predicted_auc_values = list(temp_score_seeds.values())\n",
    "mean_auc_scores = np.mean(np.array(predicted_auc_values))\n",
    "std_auc_scores = np.std(np.array(predicted_auc_values))\n",
    "print('running 5fold cv for 100 times resulted an AUC average score of {a:1.3f} with {b:1.3f} standard deviation' \\\n",
    "      .format(a=mean_auc_scores,b=std_auc_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findind the best performance among different seeds\n",
    "max_auc_val = max(predicted_auc_values)\n",
    "max_auc_val_index = predicted_auc_values.index(max_auc_val)\n",
    "best_seed_val = employed_seed_vals[max_auc_val_index]\n",
    "best_seed_experiment = seed_summary[best_seed_val]\n",
    "test_set_scores = np.zeros((len(x_test), n_fold_split), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantifying the performance of the best model over the test set.\n",
    "cnd = 0\n",
    "for key, val in best_seed_experiment.items():\n",
    "    \n",
    "    test_set_scores[:,cnd]= val['test_pred_prob']\n",
    "    cnd +=1\n",
    "mean_test_scores = np.mean(test_set_scores, axis=1)\n",
    "\n",
    "\n",
    "unseen_test_auc = roc_auc_score(y_test, mean_test_scores)\n",
    "print('The AUC value of test set is {}'.format(unseen_test_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average AUC value over validation set is {a:1.3} and the predicted AUC value for test set is {b:1.3}'.format(a=mean_auc_scores,b=unseen_test_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the performance of the model over validation and test sets is almost similar. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
